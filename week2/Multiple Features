Multiple Features
Note: [7:25 - θ^T  is a 1 by (n+1) matrix and not an (n+1) by 1 matrix]

Linear regression with multiple variables is also known as "multivariate linear regression".
We now introduce notation for equations where we can have any number of input variables.

x_j^(i)=value of feature j in the ith training example
x^(i) = the input (features) of the ith training example
m =the number of training examples
n =the number of features

The multivariable form of the hypothesis function accommodating these multiple features is as follows:

hθ(x) = θ0 + θ1x1 + θ2x2 + θ3x3 + ... + θnxn

n order to develop intuition about this function, we can think about θ0 as the basic price of a house, θ1 as the price per square meter,
θ2 as the price per floor, etc

Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:

hθ(x) = [θ0 θ1 ... θn] [x0     = θ^T * x 
                        x1
                        ...
                        xn]

This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more.
